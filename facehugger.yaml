# facehugger Huggingface Manifest file
#
# This file defines model repositories and optional include/exclude/ref settings.
# The `facehugger` python module parses this file and does the equivalent of `hf download <repo> [--include <pattern>] [--exclude <pattern>] [--revision <ref>]` for each model.
#
#
# Example lock file:
#
# ```yaml
# models:
#   # Example just repo
#   - repo: my-org/my-model
#
#   # Example repo + include pattern
#   - repo: my-org/my-model
#     include: path/to/files/*
#
#   # Example repo + include + exclude + ref
#   - repo: my-org/my-model
#     include: path/to/files/*
#     exclude: path/to/ignore/*
#     ref: v1.2.3
#```

models:
  - repo: bartowski/kldzj_gpt-oss-120b-heretic-GGUF
    include: kldzj_gpt-oss-120b-heretic-bf16/*

  # Jesse uses BF16, but I want to test Q8_0 because I think it will be faster on the Strix Halo
  - repo: unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF
    include: Qwen3-Coder-30B-A3B-Instruct-Q8_0.gguf

  - repo: unsloth/Qwen3-Coder-Next-GGUF
    include: UD-Q8_K_XL/*

  - repo: unsloth/GLM-4.7-Flash-GGUF
    include: GLM-4.7-Flash-UD-Q8_K_XL.gguf


  #- repo: deepseek-ai/DeepSeek-R1
  #  include: DeepSeek-R1-Distill-Llama-70B-Q4_K_M.gguf
  #- repo: kldzj/gpt-oss-120b
  #  include: kldzj_gpt-oss-120b-heretic-bf16-00001-of-00002.gguf
  #- repo: meta-llama/Llama-4-Scout
  #  include: Llama-4-Scout-17B-16E-Instruct-Q4_K_M-00001-of-00002.gguf

  # 128K means specially prepared version supporting 128K tokens natively via YaRN position encoding integration
  # Jesse is using the Q6_K_XL quant - VonSnickety/framework-ai-cachyos
  - repo: unsloth/Qwen3-4B-128K-GGUF
    include: Qwen3-4B-128K-UD-Q6_K_XL.gguf
  # I want to try the Q8_K_XL quant
  - repo: unsloth/Qwen3-4B-128K-GGUF
    include: Qwen3-4B-128K-UD-Q8_K_XL.gguf

